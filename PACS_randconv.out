nohup: ignoring input
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:962: UserWarning: Overwriting vit_small_patch16_224 in registry with domainbed.visiontransformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:975: UserWarning: Overwriting vit_base_patch16_224 in registry with domainbed.visiontransformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:987: UserWarning: Overwriting vit_base_patch16_384 in registry with domainbed.visiontransformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:998: UserWarning: Overwriting vit_base_patch32_384 in registry with domainbed.visiontransformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1009: UserWarning: Overwriting vit_large_patch16_224 in registry with domainbed.visiontransformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1020: UserWarning: Overwriting vit_large_patch16_384 in registry with domainbed.visiontransformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1031: UserWarning: Overwriting vit_large_patch32_384 in registry with domainbed.visiontransformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1057: UserWarning: Overwriting vit_small_resnet26d_224 in registry with domainbed.visiontransformer.vit_small_resnet26d_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_resnet26d_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1077: UserWarning: Overwriting vit_base_resnet26d_224 in registry with domainbed.visiontransformer.vit_base_resnet26d_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_resnet26d_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1087: UserWarning: Overwriting vit_base_resnet50d_224 in registry with domainbed.visiontransformer.vit_base_resnet50d_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_resnet50d_224(pretrained=False, **kwargs):
Not launched: ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s2 ('PACS', 'RandConv_CNN', [0, 1, 3], 0)
Not launched: ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s3 ('PACS', 'RandConv_CNN', [0, 1, 3], 0)
2 jobs: 0 done, 0 incomplete, 2 not launched.
python -m domainbed.scripts.train --algorithm RandConv_CNN --data_dir /media/SSD2/Dataset --dataset PACS --holdout_fraction 0.2 --hparams '{"batch_size":32,"lr":0.0001,"resnet_dropout":0.0,"weight_decay":0.0,"resnet18":true,"fixed_featurizer":false,"empty_fc":true}' --hparams_seed 0 --output_dir ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s2 --seed 1239948206 --task domain_generalization --test_envs 0 1 3 --trial_seed 2

python -m domainbed.scripts.train --algorithm RandConv_CNN --data_dir /media/SSD2/Dataset --dataset PACS --holdout_fraction 0.2 --hparams '{"batch_size":32,"lr":0.0001,"resnet_dropout":0.0,"weight_decay":0.0,"resnet18":true,"fixed_featurizer":false,"empty_fc":true}' --hparams_seed 0 --output_dir ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s3 --seed 73519977 --task domain_generalization --test_envs 0 1 3 --trial_seed 3
About to delete 0 jobs.
Good to go
Deleting...
Deleted 0 jobs!
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:962: UserWarning: Overwriting vit_small_patch16_224 in registry with domainbed.visiontransformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:975: UserWarning: Overwriting vit_base_patch16_224 in registry with domainbed.visiontransformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:987: UserWarning: Overwriting vit_base_patch16_384 in registry with domainbed.visiontransformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:998: UserWarning: Overwriting vit_base_patch32_384 in registry with domainbed.visiontransformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1009: UserWarning: Overwriting vit_large_patch16_224 in registry with domainbed.visiontransformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1020: UserWarning: Overwriting vit_large_patch16_384 in registry with domainbed.visiontransformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1031: UserWarning: Overwriting vit_large_patch32_384 in registry with domainbed.visiontransformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1057: UserWarning: Overwriting vit_small_resnet26d_224 in registry with domainbed.visiontransformer.vit_small_resnet26d_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_resnet26d_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1077: UserWarning: Overwriting vit_base_resnet26d_224 in registry with domainbed.visiontransformer.vit_base_resnet26d_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_resnet26d_224(pretrained=False, **kwargs):
/media/SSD2/kavindya/Model/TFS-ViT_Token-level_Feature_Stylization/domainbed/visiontransformer.py:1087: UserWarning: Overwriting vit_base_resnet50d_224 in registry with domainbed.visiontransformer.vit_base_resnet50d_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_resnet50d_224(pretrained=False, **kwargs):
  0%|          | 0/2 [00:00<?, ?it/s]                                     Not launched: ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s2 ('PACS', 'RandConv_CNN', [0, 1, 3], 0)
Not launched: ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s3 ('PACS', 'RandConv_CNN', [0, 1, 3], 0)
2 jobs: 0 done, 0 incomplete, 2 not launched.
python -m domainbed.scripts.train --algorithm RandConv_CNN --data_dir /media/SSD2/Dataset --dataset PACS --holdout_fraction 0.2 --hparams '{"batch_size":32,"lr":0.0001,"resnet_dropout":0.0,"weight_decay":0.0,"resnet18":true,"fixed_featurizer":false,"empty_fc":true}' --hparams_seed 0 --output_dir ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s2 --seed 1239948206 --task domain_generalization --test_envs 0 1 3 --trial_seed 2

python -m domainbed.scripts.train --algorithm RandConv_CNN --data_dir /media/SSD2/Dataset --dataset PACS --holdout_fraction 0.2 --hparams '{"batch_size":32,"lr":0.0001,"resnet_dropout":0.0,"weight_decay":0.0,"resnet18":true,"fixed_featurizer":false,"empty_fc":true}' --hparams_seed 0 --output_dir ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s3 --seed 73519977 --task domain_generalization --test_envs 0 1 3 --trial_seed 3
About to launch 2 jobs.
Good to go
Launching...
Making job directories:
WARNING: using experimental multi_gpu_launcher.
Launched 2 jobs!
Environment:
	Python: 3.8.8
	PyTorch: 2.0.1+cu117
	Torchvision: 0.15.2+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.20.1
	PIL: 8.2.0
Args:
	algorithm: RandConv_CNN
	checkpoint_freq: None
	data_dir: /media/SSD2/Dataset
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"batch_size":32,"lr":0.0001,"resnet_dropout":0.0,"weight_decay":0.0,"resnet18":true,"fixed_featurizer":false,"empty_fc":true}
	hparams_seed: 0
	output_dir: ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s2
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 1239948206
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0, 1, 3]
	trial_seed: 2
	uda_holdout_fraction: 0
HParams:
	alpha_max: 1.0
	alpha_min: 0.0
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	consistency_loss_w: 10.0
	custom_train: 0
	custom_train_val: False
	custom_val: 0
	data_augmentation: False
	digits: True
	empty_fc: True
	empty_head: False
	eval: False
	fixed_featurizer: False
	identity_prob: 0.0
	invariant_loss: True
	lr: 0.0001
	mixing: True
	nonlinear_classifier: False
	randomize_kernel: True
	resnet18: True
	resnet_dropout: 0.0
	test_env: [0, 1, 3]
	weight_decay: 0.0
	weight_decay_d: 0.0
device: cuda
Current cuda device  0
[INFO] NOT Doing Data Augmentation
[INFO] NOT Doing Data Augmentation
[INFO] NOT Doing Data Augmentation
[INFO] NOT Doing Data Augmentation
env  0  :  AA_P  in  1336  out  334
env  1  :  AR  in  1639  out  409
env  2  :  C  in  1876  out  468
env  3  :  S  in  3144  out  785
Environment:
	Python: 3.8.8
	PyTorch: 2.0.1+cu117
	Torchvision: 0.15.2+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.20.1
	PIL: 8.2.0
Args:
	algorithm: RandConv_CNN
	checkpoint_freq: None
	data_dir: /media/SSD2/Dataset
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"batch_size":32,"lr":0.0001,"resnet_dropout":0.0,"weight_decay":0.0,"resnet18":true,"fixed_featurizer":false,"empty_fc":true}
	hparams_seed: 0
	output_dir: ./Results/PACS/RandConv_CNN_Clipped/ResNet_18/t013_s3
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 73519977
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0, 1, 3]
	trial_seed: 3
	uda_holdout_fraction: 0
HParams:
	alpha_max: 1.0
	alpha_min: 0.0
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	consistency_loss_w: 10.0
	custom_train: 0
	custom_train_val: False
	custom_val: 0
	data_augmentation: False
	digits: True
	empty_fc: True
	empty_head: False
	eval: False
	fixed_featurizer: False
	identity_prob: 0.0
	invariant_loss: True
	lr: 0.0001
	mixing: True
	nonlinear_classifier: False
	randomize_kernel: True
	resnet18: True
	resnet_dropout: 0.0
	test_env: [0, 1, 3]
	weight_decay: 0.0
	weight_decay_d: 0.0
device: cuda
Current cuda device  0
[INFO] NOT Doing Data Augmentation
[INFO] NOT Doing Data Augmentation
[INFO] NOT Doing Data Augmentation
[INFO] NOT Doing Data Augmentation
env  0  :  AA_P  in  1336  out  334
env  1  :  AR  in  1639  out  409
env  2  :  C  in  1876  out  468
env  3  :  S  in  3144  out  785
/home/kavindya/anaconda3/envs/ViT_DGbed_2/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/kavindya/anaconda3/envs/ViT_DGbed_2/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
moving to the classifier
full_model_tuning_only
/home/kavindya/anaconda3/envs/ViT_DGbed_2/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/kavindya/anaconda3/envs/ViT_DGbed_2/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
moving to the classifier
full_model_tuning_only
+ checkpoint_freq: 300
+ checkpoint_freq: 300
Best model upto now
env0_in_acc   env0_in_loss  env0_out_acc  env0_out_los  env1_in_acc   env1_in_loss  env1_out_acc  env1_out_los  env2_in_acc   env2_in_loss  env2_out_acc  env2_out_los  env3_in_acc   env3_in_loss  env3_out_acc  env3_out_los  epoch         loss          mem_gb        step          step_time     train_acc    
0.3038922156  1.8977248777  0.2664670659  1.9057803551  0.1757169005  2.0090523225  0.1931540342  1.9797543883  0.2457356077  1.8192532698  0.2243589744  1.7960041463  0.1062340967  2.1442979145  0.1044585987  2.1209231104  0.0000000000  2.1072928905  2.2057385445  0             2.0022084713  0.2187500000 
Best model upto now
env0_in_acc   env0_in_loss  env0_out_acc  env0_out_los  env1_in_acc   env1_in_loss  env1_out_acc  env1_out_los  env2_in_acc   env2_in_loss  env2_out_acc  env2_out_los  env3_in_acc   env3_in_loss  env3_out_acc  env3_out_los  epoch         loss          mem_gb        step          step_time     train_acc    
0.2552395210  1.9143157764  0.2694610778  1.9386206071  0.2269676632  1.9515334826  0.2371638142  1.9538299739  0.2500000000  1.8733302752  0.2606837607  1.8789975047  0.1224554707  2.0336287212  0.1286624204  1.9965310097  0.0000000000  2.4059019089  2.2057399750  0             1.2529752254  0.0937500000 
Best model upto now
0.6646706587  1.2461337610  0.7005988024  1.1383552949  0.5393532642  1.7492150435  0.5256723716  1.6529279351  0.9701492537  0.0982667764  0.8696581197  0.3694769219  0.6911577608  0.9038901973  0.6789808917  0.8838229073  7.1856287425  0.6032124646  2.3151617050  300           0.1368578394  0.8239583333 
Best model upto now
0.6691616766  1.1583376581  0.6736526946  1.0881869396  0.5466748017  1.6125726241  0.5672371638  1.5229955912  0.9776119403  0.0816465343  0.8354700855  0.4178164974  0.6867048346  1.0435255790  0.6687898089  1.1550126501  7.1856287425  0.5390375232  2.3151631355  300           0.1376061710  0.8509375000 
Best model upto now
0.6302395210  1.3565881740  0.7095808383  1.1342828274  0.5899938987  1.6326387295  0.5867970660  1.4487368464  0.9898720682  0.0309685358  0.8632478632  0.4396255091  0.6574427481  1.3281857634  0.6484076433  1.4090192488  14.371257485  0.1542518823  2.3151617050  600           0.1379523365  0.9760416667 
0.5591317365  1.9236994332  0.6347305389  1.6939899127  0.5381330079  2.0598027523  0.5647921760  1.9616966546  0.9946695096  0.0255845487  0.8504273504  0.4958967268  0.6708015267  1.1628914618  0.6777070064  1.2047423720  14.371257485  0.1256350090  2.3151631355  600           0.1384767278  0.9813541667 
0.6676646707  1.4829943072  0.6736526946  1.5557785034  0.5710799268  1.9180274835  0.5501222494  1.9718596637  0.9936034115  0.0174173520  0.8696581197  0.5307703018  0.6959287532  1.3840384030  0.7019108280  1.3878853491  21.556886227  0.0854660554  2.3155336380  900           0.1380439377  0.9880208333 
Best model upto now
0.6856287425  1.3943018371  0.7215568862  1.2728618383  0.5784014643  1.9511069059  0.6577017115  1.6140996516  0.9941364606  0.0197936175  0.8589743590  0.5726954043  0.7245547074  1.2556323814  0.7006369427  1.3915315015  21.556886227  0.0754858166  2.3151631355  900           0.1386663032  0.9912500000 
Best model upto now
0.7514970060  1.0776621754  0.7395209581  1.1783907016  0.6223306894  1.7563853631  0.6136919315  1.8264211714  0.9973347548  0.0089162817  0.8482905983  0.6348387599  0.7108778626  1.4706592226  0.7057324841  1.4209461808  28.742514970  0.0456391881  2.3155336380  1200          0.1382127786  0.9948958333 
0.6833832335  1.5091592724  0.6826347305  1.5209785700  0.5338621110  2.4376835273  0.6039119804  2.0189842880  0.9925373134  0.0189029190  0.8311965812  0.6402730942  0.6933842239  1.4539844728  0.6929936306  1.4506708298  28.742514970  0.0638370998  2.3155293465  1200          0.1384299254  0.9917708333 
0.6534431138  2.1157577363  0.6586826347  2.0325684945  0.5051860891  2.7517501024  0.4889975550  2.7037411928  0.9989339019  0.0039180649  0.8760683761  0.5423395783  0.6599872774  2.4498565769  0.6560509554  2.4930050373  35.928143712  0.0331489294  2.3161993027  1500          0.1377971800  0.9968750000 
Best model upto now
0.6736526946  2.0935470299  0.7335329341  1.9281420310  0.5375228798  3.1016748318  0.5672371638  2.6224283874  0.9989339019  0.0024426537  0.8760683761  0.6016198844  0.6927480916  1.8962035465  0.6968152866  2.1291222743  35.928143712  0.0419580422  2.3155293465  1500          0.1393863130  0.9953125000 
0.5726047904  2.5299496000  0.5449101796  2.4432677428  0.4826113484  2.9967474020  0.4352078240  3.4012529254  0.9978678038  0.0124506243  0.8632478632  0.4839363322  0.6415394402  1.9442155647  0.6331210191  1.8547651597  43.113772455  0.0602048256  2.3161993027  1800          0.1384073369  0.9936458333 
0.1160179641  nan           0.1017964072  nan           0.1891397193  nan           0.1687041565  nan           0.1631130064  nan           0.1773504274  nan           0.1962468193  nan           0.1974522293  nan           43.113772455  nan           2.3161993027  1800          0.1370832896  0.3984375000 
0.1160179641  nan           0.1017964072  nan           0.1891397193  nan           0.1687041565  nan           0.1631130064  nan           0.1773504274  nan           0.1962468193  nan           0.1974522293  nan           50.299401197  nan           2.3161993027  2100          0.1363009381  0.1594791667 
0.6399700599  2.4174514250  0.6227544910  2.5199891726  0.4844417328  3.8478335601  0.4816625917  3.6932087541  0.9984008529  0.0139840870  0.8931623932  0.4963358864  0.6771628499  2.4422464561  0.6777070064  2.3862877062  50.299401197  0.0296261388  2.3161993027  2100          0.1383086411  0.9966666667 
0.1160179641  nan           0.1017964072  nan           0.1891397193  nan           0.1687041565  nan           0.1631130064  nan           0.1773504274  nan           0.1962468193  nan           0.1974522293  nan           57.485029940  nan           2.3161993027  2400          0.1365612110  0.1627083333 
0.1100299401  nan           0.1257485030  nan           0.1842586943  nan           0.1882640587  nan           0.1668443497  nan           0.1623931624  nan           0.2019720102  nan           0.1745222930  nan           57.485029940  nan           2.3161993027  2400          0.1382350977  0.9041666667 
0.1160179641  nan           0.1017964072  nan           0.1891397193  nan           0.1687041565  nan           0.1631130064  nan           0.1773504274  nan           0.1962468193  nan           0.1974522293  nan           64.670658682  nan           2.3161993027  2700          0.1358711441  0.1640625000 
0.1100299401  nan           0.1257485030  nan           0.1842586943  nan           0.1882640587  nan           0.1668443497  nan           0.1623931624  nan           0.2019720102  nan           0.1745222930  nan           64.670658682  nan           2.3161993027  2700          0.1356855623  0.1659375000 
0.1160179641  nan           0.1017964072  nan           0.1891397193  nan           0.1687041565  nan           0.1631130064  nan           0.1773504274  nan           0.1962468193  nan           0.1974522293  nan           71.856287425  nan           2.3161993027  3000          0.1364957666  0.1619791667 
0.1100299401  nan           0.1257485030  nan           0.1842586943  nan           0.1882640587  nan           0.1668443497  nan           0.1623931624  nan           0.2019720102  nan           0.1745222930  nan           71.856287425  nan           2.3161993027  3000          0.1365407332  0.1667708333 
0.1160179641  nan           0.1017964072  nan           0.1891397193  nan           0.1687041565  nan           0.1631130064  nan           0.1773504274  nan           0.1962468193  nan           0.1974522293  nan           79.041916167  nan           2.3161993027  3300          0.1367235112  0.1601041667 
0.1100299401  nan           0.1257485030  nan           0.1842586943  nan           0.1882640587  nan           0.1668443497  nan           0.1623931624  nan           0.2019720102  nan           0.1745222930  nan           79.041916167  nan           2.3161993027  3300          0.1364339463  0.1673958333 
Terminated
Terminated
